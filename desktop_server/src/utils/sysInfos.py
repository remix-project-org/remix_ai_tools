# Check if gpu is available 
# determine max context size for both models


import llama_cpp